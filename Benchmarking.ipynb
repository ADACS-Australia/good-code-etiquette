{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harley Wood School for Astronomy 2019 \n",
    "\n",
    "<img src=\"https://research.smp.uq.edu.au/asa2019/static/asa19/img/HWSA2019-logo.png\" width=300>\n",
    "\n",
    "## Part IV - Timing and profiling code snippets to help write faster code\n",
    "\n",
    "In this session, you will learn how to measure the runtime \n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [When and What to Optimise](#When-and-What-to-Optimise)\n",
    "2. [Timing Code Snippets](#Timing-Code-Snippets)\n",
    "3. [Profiling Code](#Profiling-Code-line-by-line)\n",
    "4. [Parallelising Code](#Parallelising-code)\n",
    "5. [Advanced Track](#Advanced-Track)\n",
    "\n",
    "\n",
    "### Required libraries\n",
    "\n",
    "This notebook uses several Python packages that come standard with the [Anaconda Python distribution](http://continuum.io/downloads). The primary libraries that we'll be using are:\n",
    "\n",
    "* **astropy**\n",
    "* **pandas**\n",
    "* **numpy**\n",
    "* **pandas**\n",
    "* **line_profiler**\n",
    "* **memory_profiler**\n",
    "* **schwimmbad** [optional]\n",
    "\n",
    "If you have created a new environment with `conda env create -f hwsa-environment.yml`, then you are all set. If not, To make sure you have all of the packages you need, install them with `conda`:\n",
    "\n",
    "    conda install [package name]\n",
    "    \n",
    "`conda` may ask you to update some of the packages if you don't have the most recent version. Allow it to do so.\n",
    "\n",
    "Alternatively, if you can install the packages with [pip](https://pip.pypa.io/en/stable/installing/) (a Python package manager):\n",
    "\n",
    "    pip install [package name]\n",
    "    \n",
    "Be sure to restart your kernel if you had to install new packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# When and What to Optimise\n",
    "\n",
    "\n",
    "**Code is read far more frequently than written**\n",
    "\n",
    "Optimisation mantras:\n",
    "\n",
    "1. [Obligatory XKCD](https://xkcd.com/1205/)\n",
    "\n",
    "2. Do not optimise unless you have to (your time is *the most precious*)\n",
    "\n",
    "3. Optimise for readability (imagine very cranky future developers, and reduce their cognitive load)\n",
    "\n",
    "4. Variety of optimisations \n",
    "    - Readability\n",
    "    - Usability\n",
    "    - Performance   \n",
    "    - Memory\n",
    "    - Lines of code (code-golf)\n",
    "    \n",
    "5. Remember, every line of code is a potential source of bugs, and future maintenance head-aches. Choose wisely!    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Code Snippets\n",
    "\n",
    "`time` magic command measures the time taken by a code snippet by running the snippet once. What are the challenges to timing a piece of code you say?\n",
    "\n",
    "- Ensure that the timing is representative of the average case\n",
    "- Get some sense of the scatter in the runtimes\n",
    "\n",
    "`timeit` magic command tries to answer both those potential issues. `timeit` will repeat a chunk of code some *N* times till a stable runtime measurement is reached. Lots of customisable options -- see the manual [here](https://docs.python.org/3/library/timeit.html)\n",
    "\n",
    "# Magic commands tips\n",
    "\n",
    "Use `%%` at the top of the code snippet (if not a function), or use `%` to invoke when timing a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#read data \n",
    "#from astropy.io.votable import parse_single_table\n",
    "#table = parse_single_table(\"async_20190630210155.vot\")\n",
    "#t = table.to_table(use_names_over_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That takes a LOOOONG time\n",
    "\n",
    "Can we reduce the total read time? Perhaps we can only read-in the specific columns that we *need*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the subset data \n",
    "from astropy.io.votable import parse_single_table\n",
    "table = parse_single_table(\"async_subset.vot\")\n",
    "t = table.to_table(use_names_over_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If you downloaded the pickle file, you can restore via this cell\n",
    "\n",
    "#import pandas as pd\n",
    "#df = pd.read_pickle(\"async_20190630210155.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data -> but this try/except will protect you from re-running a very SLOOW process (un-necessarily)\n",
    "try:\n",
    "    table\n",
    "except NameError:\n",
    "    from astropy.io.votable import parse_single_table\n",
    "    columns = ['phot_g_mean_mag', 'parallax']\n",
    "    table = parse_single_table(\"async_subset.vot\", columns=columns)\n",
    "    print(\"Done reading table\")\n",
    "\n",
    "t = table.to_table(use_names_over_ids=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting the data\n",
    "\n",
    "Now that you have managed to read-in the data (either the full or subset one), let's convert this to a more convenient data-frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = t.to_pandas()\n",
    "\n",
    "#check the data frame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "#convert to pandas df and calculate absolute mag\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "df['mg'] = 0\n",
    "df['dist'] = 0\n",
    "\n",
    "for c, v in enumerate(df['phot_g_mean_mag']):\n",
    "    \n",
    "    p =df.loc[c,'parallax']\n",
    "    if p>0:\n",
    "        df.loc[c,'mg'] = v + 5 * log10(p) - 10\n",
    "        df.loc[c,'dist'] = 1000/p\n",
    "    else:\n",
    "        df.loc[c,'mg'] = np.nan\n",
    "        df.loc[c,'dist'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loops are slow \n",
    "\n",
    "In python, `for` loops are slow. So you should try to avoid them whenever possible. In the following example, we will try to create an array of integers, where each element is the square of the index. The goal would be to create the fastest possible implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_list_creation(N):    \n",
    "    squares = []\n",
    "    for i in range(N):\n",
    "        squares.append(i**2)\n",
    "        \n",
    "    return squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_comprehension(N):\n",
    "    squares = [i**2 for i in range(N)]\n",
    "    return squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit basic_list_creation(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit list_comprehension(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allocating and re-allocating memory is slow\n",
    "\n",
    "If you know the size of the array, then pre-allocate the array/list, and then fill in at the appropriate index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prealloc_list(N):\n",
    "    squares = [0]*N\n",
    "    for i in range(N):\n",
    "        squares[i] = i*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit prealloc_list(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy is faster\n",
    "\n",
    "Let's try to write this \"square\" function in `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_squares(N):\n",
    "    import numpy as np\n",
    "    squares = np.empty(N)\n",
    "    for i in range(N):\n",
    "        squares[i] = i*i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numpy_squares(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For advanced users -> implement this function. Extra points if you can make `numpy_only` more than 20x faster\n",
    "def numpy_only(N):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit numpy_only(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to processing GAIA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_attempt_at_abs_mag_and_dist(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    df['mg2'] = 0\n",
    "    df['dist2'] = 0\n",
    "\n",
    "    for c, v in enumerate(df['phot_g_mean_mag']):\n",
    "\n",
    "        p =df.loc[c,'parallax']\n",
    "        if p>0:\n",
    "            df.loc[c,'mg2'] = v + 5 * math.log10(p) - 10\n",
    "            df.loc[c,'dist2'] = 1000/p\n",
    "        else:\n",
    "            df.loc[c,'mg2'] = np.nan\n",
    "            df.loc[c,'dist2'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit second_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For loops are sloooow in python\n",
    "\n",
    "But we do need to loop - how do we that? List comprehensions to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas df and calculate absolute mag\n",
    "def third_attempt_at_abs_mag_and_dist(df):\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    apparent_mags = df['phot_g_mean_mag']\n",
    "    parallax = df['parallax']\n",
    "    abs_mags = [mag + 5*math.log10(dist) - 10 if dist > 0 else np.nan for mag, dist in zip(apparent_mags, parallax)]\n",
    "    dists = [1000.0/d if d > 0 else np.nan for d in parallax ]\n",
    "    \n",
    "    df['mg3'] = abs_mags\n",
    "    df['dist3'] = dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit third_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas df and calculate absolute mag\n",
    "def fourth_attempt_at_abs_mag_and_dist(df):\n",
    "    import numpy as np\n",
    "\n",
    "    apparent_mags = df['phot_g_mean_mag']\n",
    "    parallax = df['parallax']\n",
    "    abs_mags = [mag + 5*np.log10(dist) - 10 if dist > 0 else np.nan for mag, dist in zip(apparent_mags, parallax)]\n",
    "    dists = [1000.0/d if d > 0 else np.nan for d in parallax ]\n",
    "    \n",
    "    df['mg4'] = abs_mags\n",
    "    df['dist4'] = dists\n",
    "\n",
    "%timeit fourth_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas df and calculate absolute mag\n",
    "def fifth_attempt_at_abs_mag_and_dist(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    apparent_mags = df['phot_g_mean_mag']\n",
    "    parallax = df['parallax']\n",
    "    abs_mags = apparent_mags + 5.0*np.log10(parallax) - 10\n",
    "    dist = 1000.0/parallax\n",
    "\n",
    "    bad_inds = (~np.isfinite(parallax) | (parallax <= 0))\n",
    "    abs_mags[bad_inds] = np.nan\n",
    "    dist[bad_inds] = np.nan\n",
    "  \n",
    "    df['mg5'] = abs_mags\n",
    "    df['dist5'] = dist\n",
    "\n",
    "%timeit fifth_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to pandas df and calculate absolute mag\n",
    "def sixth_attempt_at_abs_mag_and_dist(df):\n",
    "    import numpy as np\n",
    "\n",
    "    apparent_mags = df['phot_g_mean_mag']\n",
    "    parallax = df['parallax']\n",
    "    \n",
    "    abs_mags = np.full_like(apparent_mags, np.nan)\n",
    "    dist = np.full_like(parallax, np.nan)\n",
    "\n",
    "    good_inds = (np.isfinite(parallax) & (parallax > 0))\n",
    "    abs_mags[good_inds] = apparent_mags[good_inds] + 5.0*np.log10(parallax[good_inds]) - 10\n",
    "    dist[good_inds] = 1000.0/parallax[good_inds]\n",
    "\n",
    "    df['mg6'] = abs_mags\n",
    "    df['dist6'] = dist\n",
    "\n",
    "%timeit sixth_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_abs_mag_and_distance(df):\n",
    "    import numpy as np\n",
    "    df['optim_abs_mag'] = df['phot_g_mean_mag'] + 5*np.log10(df['parallax']) - 10\n",
    "    df['optim_dist'] = 1000.0/df['parallax']\n",
    "       \n",
    "%timeit  add_abs_mag_and_distance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling Code line by line\n",
    "\n",
    "So far we have written code and timed entire chunks of code. What if you wanted to know how much time each line of your code takes?\n",
    "\n",
    "There is a magic command for that -- `line_profiler`. But now you do have to load this magic command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_attempt_at_abs_mag_and_dist(df):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import math\n",
    "\n",
    "    df['mg2'] = 0\n",
    "    df['dist2'] = 0\n",
    "\n",
    "    for c, v in enumerate(df['phot_g_mean_mag']):\n",
    "\n",
    "        p =df.loc[c,'parallax']\n",
    "        if p>0:\n",
    "            df.loc[c,'mg2'] = v + 5 * math.log10(p) - 10\n",
    "            df.loc[c,'dist2'] = 1000/p\n",
    "        else:\n",
    "            df.loc[c,'mg2'] = np.nan\n",
    "            df.loc[c,'dist2'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f second_attempt_at_abs_mag_and_dist second_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f third_attempt_at_abs_mag_and_dist third_attempt_at_abs_mag_and_dist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f add_abs_mag_and_distance add_abs_mag_and_distance(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An application of line-profiling\n",
    "\n",
    "## Finding Unique Values\n",
    "\n",
    "Say you might find the number of unique values in an HUGE array, where potentially many elements are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_values(arr):\n",
    "    unique_vals = []\n",
    "    \n",
    "    for x in arr:\n",
    "        if x not in unique_vals:\n",
    "            unique_vals.append(x)\n",
    "    \n",
    "    return unique_vals    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 10000\n",
    "nrepeats = 3000\n",
    "dups = np.repeat(np.arange(N), nrepeats)\n",
    "\n",
    "%timeit find_unique_values(dups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Use line-profiler oon this above code and come up with faster solutions (that still use `for` loops). Bonus points for solutions without `for` loops. \n",
    "\n",
    "# Idiomatic python \n",
    "\n",
    "`if x in large_array` is `O(len(large_array))` if `large_array` is a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How about profiling for memory usage\n",
    "\n",
    "What is the amount of memory required per line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file mprun_demo.py\n",
    "def read_votable():\n",
    "    if not table:\n",
    "        from astropy.io.votable import parse_single_table\n",
    "        columns = ['phot_g_mean_mag', 'parallax']\n",
    "        table = parse_single_table(\"async_20190630210155.vot\", columns=columns)\n",
    "        print(\"Done reading table\")\n",
    "    return table.to_table(use_names_over_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mprun_demo import read_votable\n",
    "%mprun -f read_votable read_votable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelising code\n",
    "\n",
    "Python, **by design**, will only run in serial. There are a couple of ways you can run code in parallel. Most commonly, you will use the `multiprocessing` module. \n",
    "\n",
    "*Note* `multiprocessing` has a large overhead for creating these new \"processes\". If you are trying to use `multiprocessing` for relatively fast code snippets, then chances are that the \"parallel\" version will be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "print(\"Number of cpus = {}\".format(multiprocessing.cpu_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    xrange\n",
    "except NameError:\n",
    "    xrange = range\n",
    "\n",
    "def squareit(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N=100000\n",
    "\n",
    "%timeit squareit(np.arange(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "from multiprocessing import Pool\n",
    "nprocs = multiprocessing.cpu_count()\n",
    "with Pool(processes=nprocs) as pool:\n",
    "    _  = pool.map(squareit, range(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "from schwimmbad import MultiPool\n",
    "\n",
    "with MultiPool(nprocs) as pool:\n",
    "    _ = list(pool.map(squareit, range(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "from schwimmbad import MultiPool\n",
    "\n",
    "with MultiPool(nprocs) as pool:\n",
    "    _ = list(pool.map(squareit, range(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MPI parallelism\n",
    "\n",
    "As long as you can write a function that operates on a certain chunk of the data, then both `multiprocessing` and `MPI` based parallelism are feasible. `mpi4py` is a convenient package that lets you use a supercomputer cluster, and in theory, your code can run significantly faster (limited by the number of cores on your supercomputer). \n",
    "\n",
    "I would recommend using the `schwimmbad` package to transparently use both `MPI` and `multiprocessing` based parallelism. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Track \n",
    "\n",
    "\n",
    "## Challenge #1: Difficulty Rating - *Medium*\n",
    "\n",
    "Perform the entire operation of calculating `absolute-mag` and `distance` in parallel. First, create the parallel interface with `multiprocessing`, and then create an MPI parallel implementation with `mpi4py`.\n",
    "\n",
    "Hint: Use the package `schwimmbad` for a customisable solution. \n",
    "\n",
    "\n",
    "## Challenge #2: Difficulty Rating - *Easy*\n",
    "\n",
    "There is an open issue on the astropy repo [issue no 8946](https://github.com/astropy/astropy/issues/8946). Figure out what causes the high-memory usage. \n",
    "\n",
    "Hint: You will need `mem_profiler` for the first challenge\n",
    "\n",
    "\n",
    "## Challenge #3: Difficulty Rating - *Medium*\n",
    "\n",
    "Open a pull-request to fix [issue no 8946](https://github.com/astropy/astropy/issues/8946)\n",
    "\n",
    "\n",
    "## Challenge #4: Difficulty Rating - *Ninja*\n",
    "\n",
    "Open a pull-request to fix [issue no 8946](https://github.com/astropy/astropy/issues/8946) *AND* to read-in only some *Nrows* of data (i.e., add a new keyword). Any addition must maintain backwards compatibility. \n",
    "\n",
    "**Disclaimer** (MS) I do not know how to do this. If you are (interested in) attempting this, please let me know - I can put you in touch with astropy developers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
